(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{425:function(e,t,o){e.exports=o.p+"assets/img/gst-overview.dbfa6b6d.png"},426:function(e,t,o){e.exports=o.p+"assets/img/gst-module-design.46bb49f1.png"},450:function(e,t,o){"use strict";o.r(t);var i=o(65),n=Object(i.a)({},(function(){var e=this,t=e.$createElement,i=e._self._c||t;return i("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[i("h1",{attrs:{id:"gestural-sound-tookit"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#gestural-sound-tookit"}},[e._v("#")]),e._v(" Gestural Sound Tookit")]),e._v(" "),i("p",[e._v("The  "),i("a",{attrs:{href:"https://github.com/ircam-ismm/Gestural-Sound-Toolkit",target:"_blank",rel:"noopener noreferrer"}},[e._v("Gestural Sound Toolkit"),i("OutboundLink")],1),e._v(", using the package "),i("code",[e._v("MuBu")]),e._v(" (available from the Cylcling'74 packages), is a set of objects to facilitate sensor processing, gesture recognition and mapping to sound synthesis engines.")]),e._v(" "),i("ul",[i("li",[i("a",{attrs:{href:"https://github.com/ircam-ismm/Gestural-Sound-Toolkit",target:"_blank",rel:"noopener noreferrer"}},[e._v("Gestural-Sound-Toolkit@github"),i("OutboundLink")],1)]),e._v(" "),i("li",[i("a",{attrs:{href:"https://nubo.ircam.fr/index.php/s/FxJYBzJtGQSNcCg",target:"_blank",rel:"noopener noreferrer"}},[e._v("Tech Report"),i("OutboundLink")],1)])]),e._v(" "),i("h2",{attrs:{id:"video"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#video"}},[e._v("#")]),e._v(" Video")]),e._v(" "),i("p",[e._v("(Soon, short video demo)")]),e._v(" "),i("h2",{attrs:{id:"introduction"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[e._v("#")]),e._v(" Introduction")]),e._v(" "),i("p",[i("img",{attrs:{src:o(425),alt:"GST module design"}})]),e._v(" "),i("p",[e._v("GST is designed to build gestural interactions with sounds, using Machine Learning (ML) methods for gesture recognition, gesture following, or gesture-sound mapping among other applications. The toolkit is built on the notion of high-level modules handling specific operations. The toolkit is comprised of four types of modules:")]),e._v(" "),i("ul",[i("li",[i("p",[i("code",[e._v("Receiver")]),e._v(" modules receive motion data from some sensing hardware. In the interface, a module for "),i("em",[e._v("R-IoT")]),e._v(" and "),i("em",[e._v("Bitalino")]),e._v(" is presented. Other modules exist to get data from the "),i("em",[e._v("Leapmotion")]),e._v(", the "),i("em",[e._v("Myo")]),e._v(", or generic OSC input streams.")])]),e._v(" "),i("li",[i("p",[i("code",[e._v("Preprocessing")]),e._v(" and "),i("code",[e._v("Analysis")]),e._v(" modules analyze and process gesture data. A "),i("em",[e._v("Filter")]),e._v(" module can be used to reduce noise. The "),i("em",[e._v("Energy")]),e._v(" module extracts gestural energy from the incoming signal. Velocity is calculated by computing the derivative. Some modules are specific to inertial sensors, as the most used hardware in our work with the toolkit.")])]),e._v(" "),i("li",[i("p",[i("code",[e._v("Machine Learning")]),e._v(" modules perform classification (e.g. gesture recognition) and regression (e.g. gesture-to-sound mapping). Classification can be static (for posture recognition) or temporal (for gesture following and real-time time warping). Similarly, regression can be static or temporal.")])]),e._v(" "),i("li",[i("p",[i("code",[e._v("Synthesis")]),e._v(" modules allow prerecorded sounds to be played and manipulated. The toolkit integrates temporal modulation (scrubbing). A trigger module allows for triggering a sound from a sound bank. A manipulation module allows sound to be sculpted and modified live as movement is performed.")])])]),e._v(" "),i("h2",{attrs:{id:"module-design"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#module-design"}},[e._v("#")]),e._v(" Module Design")]),e._v(" "),i("p",[e._v("As an example, the figure below depicts the moving average module, called ["),i("code",[e._v("sp.movingaverage.maxpat")]),e._v("]. On the left, the patcher is shown in edit (or implementation) mode. It has two inputs and one output. In between, the patcher implements a moving average filter using some components from the MuBu library. Elements of the patch highlighted in pink are the ones remaining in the interface, depicted on the right of the Figure. The patcher opens in presentation mode by default (right of Figure below). Each module is independent from other modules. Therefore, extending the toolkit is made easy. One can create an arbitrary patcher and can follow the guidelines for its look in presentation mode, which defines its interface.\n"),i("img",{attrs:{src:o(426),alt:"GST module design"}})]),e._v(" "),i("h2",{attrs:{id:"typical-use-cases"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#typical-use-cases"}},[e._v("#")]),e._v(" Typical use cases")]),e._v(" "),i("h3",{attrs:{id:"use-case-1-pedagogical-tool-for-gestural-sound-interaction"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#use-case-1-pedagogical-tool-for-gestural-sound-interaction"}},[e._v("#")]),e._v(" Use case 1: Pedagogical tool for gestural sound interaction")]),e._v(" "),i("p",[e._v("Ihe toolkit has been used yearly in a pedagogical context. We describe this context and report observations made on the impact of the design.")]),e._v(" "),i("p",[e._v("We used the GST in the context of a course on designing gesture-based interactions with sounds. The course was part of a one-year curriculum on movement computing using machine learning and artificial intelligence. The course was given to young professionals who chose to dedicate one year to follow this curriculum in order to acquire skills in movement computing. Although our class was not specifically about ML, designing gesture-based interaction can involve gesture recognition, or gesture-to-sound regression. The pedagogical objectives were:")]),e._v(" "),i("ul",[i("li",[e._v("Teaching basis in movement control and learning, especially in continuous vs discrete actions, feedback and feedforward mechanisms in motor control, and the law of practice.")]),e._v(" "),i("li",[e._v("Exploring parametric sound synthesis, which includes a quick overview of sound synthesis and specific focus on granular and concatenative synthesis applied to recorded sounds.")]),e._v(" "),i("li",[e._v("Understanding gesture-sound interaction, which includes interaction design methodology, the notion of mappings and its design.")]),e._v(" "),i("li",[e._v("Creating and implementing an interactive scenario, which involves the development of such scenario, motivating design choices, and showing a working prototype at the end of the course.")])]),e._v(" "),i("h3",{attrs:{id:"use-case-2-research-through-design-rapid-prototyping"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#use-case-2-research-through-design-rapid-prototyping"}},[e._v("#")]),e._v(" Use case 2: Research through design - rapid prototyping")]),e._v(" "),i("p",[e._v("Building a tool that can be used by designers")]),e._v(" "),i("p",[e._v("GST was initially developed as part of a research project in 2013. In this project we were organising workshops with designers in which they were guided in the design of embodied interaction with sounds. The detailed of these workshops can be found in "),i("a",{attrs:{href:"https://research.gold.ac.uk/id/eprint/11418/1/caramiaux2015form.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("(Carmiaux et al. 2015)"),i("OutboundLink")],1),e._v(". In this series of workshops, we developed a specific methodology where participants were guided through the design and the implementation of an interactive scenario involving gesture control of sound synthesis. In the methodology, participants were invited to start from the sound (the feedback), as opposed to start from the gesture (the input). The motivation behind this choice is to allow designers to be led by sonic affordances, that is to say the way sounds may involve associated actions and body movements, in the design of the interactive scenario. This was the ideation phase, which was followed by a realization phase where participants used the GST to implement their scenarios, after a quick tutorial to help them handle the tool. The motivation behind the design of the toolkit was to provide designers with a tool that allows them to realize their own, some unrealistic, project involving gesture-based interaction with sound. Our objective was to build a tool that includes designers with no programming experience, while enabling versatility, which means that designers have to not feel limited in the scope of projects they would like to develop")]),e._v(" "),i("h2",{attrs:{id:"credits"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#credits"}},[e._v("#")]),e._v(" Credits")]),e._v(" "),i("blockquote",[i("p",[e._v("V1 Contributors:")])]),e._v(" "),i("p",[e._v("AVI Group Goldsmiths College (Baptiste Caramiaux, Alessandro Altavilla)")]),e._v(" "),i("p",[e._v("IRCAM-Centre Pompidou (Mubu: Riccardo Borghesi, Diemo Schwarz, Norbert Schnell, Frédéric Bevilacqua, Jules Françoise)")]),e._v(" "),i("p",[e._v("EAVI website: eavi.goldsmithsdigital.com, (c) 2015 EAVI Group, Goldsmiths College, University of London")]),e._v(" "),i("blockquote",[i("p",[e._v("V2 Contributors:")])]),e._v(" "),i("p",[e._v("STMS Lab IRCAM CNRS Sorbonne Université (Frédéric Bevilacqua, Riccardo Borghesi, Diemo Schwarz, Victor Paredes)")]),e._v(" "),i("p",[e._v("ISIR Sorbonne Université (Baptiste Caramiaux)")]),e._v(" "),i("p",[e._v("LIMSI CNRS (Jules Françoise)")]),e._v(" "),i("p",[e._v("Acknowledgement: "),i("a",{attrs:{href:"https://element-project.ircam.fr",target:"_blank",rel:"noopener noreferrer"}},[e._v("ELEMENT "),i("OutboundLink")],1),e._v(" (ANR-18-CE33-0002)")]),e._v(" "),i("h2",{attrs:{id:"publications"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#publications"}},[e._v("#")]),e._v(" Publications")]),e._v(" "),i("ul",[i("li",[e._v("Caramiaux, B., Altavilla, A., Françoise, F., Bevilacqua, F., (2022). Gestural Sound Toolkit: Reflections on an Interactive Design Project, NIME Demos, 2022")]),e._v(" "),i("li",[e._v("Caramiaux, B., Altavilla, A., Françoise, J., & Bevilacqua, F. (2022). Gestural Sound Toolkit: Reflections on an Interactive Design Project. International Conference on New Interfaces for Musical Expression. Retrieved from https://nime.pubpub.org/pub/vpgn52hr")]),e._v(" "),i("li",[e._v("Caramiaux, B., Altavilla, A., Pobiner, S. G., & Tanaka, A. (2015, April). Form follows sound: designing interactions from sonic memories. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (pp. 3943-3952).")]),e._v(" "),i("li",[e._v("Françoise, J., Schnell, N., Borghesi, R., & Bevilacqua, F. (2014, June). Probabilistic models for designing motion and sound relationships. In Proceedings of the 2014 international conference on new interfaces for musical expression (pp. 287-292).")]),e._v(" "),i("li",[e._v("Schnell, N., Röbel, A., Schwarz, D., Peeters, G., & Borghesi, R. (2009, August). MuBu and friends–assembling tools for content based real-time interactive audio processing in Max/MSP. In ICMC.")])])])}),[],!1,null,null,null);t.default=n.exports}}]);